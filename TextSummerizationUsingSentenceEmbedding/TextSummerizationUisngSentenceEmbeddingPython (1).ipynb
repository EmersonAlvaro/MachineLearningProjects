{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2019.3.12)\n",
      "Requirement already satisfied: talon in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: scikit-learn>=0.16.1 in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from talon) (0.19.1)\n",
      "Requirement already satisfied: regex>=1 in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from talon) (2019.3.12)\n",
      "Requirement already satisfied: cchardet>=0.3.5 in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from talon) (2.1.4)\n",
      "Requirement already satisfied: cssselect in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from talon) (1.0.3)\n",
      "Requirement already satisfied: html5lib in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from talon) (1.0.1)\n",
      "Requirement already satisfied: lxml>=2.3.3 in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from talon) (4.1.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from talon) (1.0.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from talon) (1.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from talon) (1.14.2)\n",
      "Requirement already satisfied: chardet>=1.0.1 in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from talon) (3.0.4)\n",
      "Requirement already satisfied: webencodings in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from html5lib->talon) (0.5.1)\n",
      "Collecting langdetect\n",
      "  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
      "Requirement already satisfied: six in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from langdetect) (1.11.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\sbha69\\AppData\\Local\\pip\\Cache\\wheels\\ec\\0c\\a9\\1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip3 install regex\n",
    "!pip3 install talon\n",
    "!pip3 install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Email_English = open(\"D:\\DataScience\\TextSummerizationUsingSentenceEmbedding\\Email_English.txt\",\"r+\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Email_EnglishText=Email_English.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Email_EnglishText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean email...\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import logging\n",
    "\n",
    "import regex as re\n",
    "\n",
    "from talon.signature.constants import (SIGNATURE_MAX_LINES,\n",
    "                                       TOO_LONG_SIGNATURE_LINE)\n",
    "from talon.utils import get_delimiter\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# regex to fetch signature based on common signature words\n",
    "RE_SIGNATURE = re.compile(r'''\n",
    "               (\n",
    "                   (?:\n",
    "                       ^[\\s]*--*[\\s]*[a-z \\.]*$\n",
    "                       |\n",
    "                       ^thanks[\\s,!]*$\n",
    "                       |\n",
    "                       ^regards[\\s,!]*$\n",
    "                       |\n",
    "                       ^cheers[\\s,!]*$\n",
    "                       |\n",
    "                       ^best[ a-z]*[\\s,!]*$\n",
    "                   )\n",
    "                   .*\n",
    "               )\n",
    "               ''', re.I | re.X | re.M | re.S)\n",
    "\n",
    "# signatures appended by phone email clients\n",
    "RE_PHONE_SIGNATURE = re.compile(r'''\n",
    "               (\n",
    "                   (?:\n",
    "                       ^sent[ ]{1}from[ ]{1}my[\\s,!\\w]*$\n",
    "                       |\n",
    "                       ^sent[ ]from[ ]Mailbox[ ]for[ ]iPhone.*$\n",
    "                       |\n",
    "                       ^sent[ ]([\\S]*[ ])?from[ ]my[ ]BlackBerry.*$\n",
    "                       |\n",
    "                       ^Enviado[ ]desde[ ]mi[ ]([\\S]+[ ]){0,2}BlackBerry.*$\n",
    "                   )\n",
    "                   .*\n",
    "               )\n",
    "               ''', re.I | re.X | re.M | re.S)\n",
    "\n",
    "# see _mark_candidate_indexes() for details\n",
    "# c - could be signature line\n",
    "# d - line starts with dashes (could be signature or list item)\n",
    "# l - long line\n",
    "RE_SIGNATURE_CANDIDATE = re.compile(r'''\n",
    "    (?P<candidate>c+d)[^d]\n",
    "    |\n",
    "    (?P<candidate>c+d)$\n",
    "    |\n",
    "    (?P<candidate>c+)\n",
    "    |\n",
    "    (?P<candidate>d)[^d]\n",
    "    |\n",
    "    (?P<candidate>d)$\n",
    "''', re.I | re.X | re.M | re.S)\n",
    "\n",
    "\n",
    "def extract_signature(msg_body):\n",
    "    '''\n",
    "    Analyzes message for a presence of signature block (by common patterns)\n",
    "    and returns tuple with two elements: message text without signature block\n",
    "    and the signature itself.\n",
    "    >>> extract_signature('Hey man! How r u?\\n\\n--\\nRegards,\\nRoman')\n",
    "    ('Hey man! How r u?', '--\\nRegards,\\nRoman')\n",
    "    >>> extract_signature('Hey man!')\n",
    "    ('Hey man!', None)\n",
    "    '''\n",
    "    try:\n",
    "        # identify line delimiter first\n",
    "        delimiter = get_delimiter(msg_body)\n",
    "        print(delimiter)\n",
    "            \n",
    "        # make an assumption\n",
    "        stripped_body = msg_body.strip()\n",
    "        phone_signature = None\n",
    "        print(stripped_body)\n",
    "        \n",
    "        # strip off phone signature\n",
    "        phone_signature = RE_PHONE_SIGNATURE.search(msg_body)\n",
    "        if phone_signature:\n",
    "            stripped_body = stripped_body[:phone_signature.start()]\n",
    "            phone_signature = phone_signature.group()\n",
    "\n",
    "        # decide on signature candidate\n",
    "        lines = stripped_body.splitlines()\n",
    "        candidate = get_signature_candidate(lines)\n",
    "        candidate = delimiter.join(candidate)\n",
    "        print(candidate)\n",
    "\n",
    "        # try to extract signature\n",
    "        signature = RE_SIGNATURE.search(candidate)\n",
    "        if not signature:\n",
    "            return (stripped_body.strip(), phone_signature)\n",
    "        else:\n",
    "            signature = signature.group()\n",
    "            # when we splitlines() and then join them\n",
    "            # we can lose a new line at the end\n",
    "            # we did it when identifying a candidate\n",
    "            # so we had to do it for stripped_body now\n",
    "            stripped_body = delimiter.join(lines)\n",
    "            stripped_body = stripped_body[:-len(signature)]\n",
    "\n",
    "            if phone_signature:\n",
    "                signature = delimiter.join([signature, phone_signature])\n",
    "\n",
    "            return (stripped_body.strip(),\n",
    "                    signature.strip())\n",
    "    except Exception:\n",
    "        log.exception('ERROR extracting signature')\n",
    "        return (msg_body, None)\n",
    "\n",
    "\n",
    "def get_signature_candidate(lines):\n",
    "    \"\"\"Return lines that could hold signature\n",
    "    The lines should:\n",
    "    * be among last SIGNATURE_MAX_LINES non-empty lines.\n",
    "    * not include first line\n",
    "    * be shorter than TOO_LONG_SIGNATURE_LINE\n",
    "    * not include more than one line that starts with dashes\n",
    "    \"\"\"\n",
    "    # non empty lines indexes\n",
    "    non_empty = [i for i, line in enumerate(lines) if line.strip()]\n",
    "\n",
    "    # if message is empty or just one line then there is no signature\n",
    "    if len(non_empty) <= 1:\n",
    "        return []\n",
    "\n",
    "    # we don't expect signature to start at the 1st line\n",
    "    candidate = non_empty[1:]\n",
    "    # signature shouldn't be longer then SIGNATURE_MAX_LINES\n",
    "    candidate = candidate[-SIGNATURE_MAX_LINES:]\n",
    "\n",
    "    markers = _mark_candidate_indexes(lines, candidate)\n",
    "    candidate = _process_marked_candidate_indexes(candidate, markers)\n",
    "\n",
    "    # get actual lines for the candidate instead of indexes\n",
    "    if candidate:\n",
    "        candidate = lines[candidate[0]:]\n",
    "        return candidate\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def _mark_candidate_indexes(lines, candidate):\n",
    "    \"\"\"Mark candidate indexes with markers\n",
    "    Markers:\n",
    "    * c - line that could be a signature line\n",
    "    * l - long line\n",
    "    * d - line that starts with dashes but has other chars as well\n",
    "    >>> _mark_candidate_lines(['Some text', '', '-', 'Bob'], [0, 2, 3])\n",
    "    'cdc'\n",
    "    \"\"\"\n",
    "    # at first consider everything to be potential signature lines\n",
    "    markers = list('c' * len(candidate))\n",
    "\n",
    "    # mark lines starting from bottom up\n",
    "    for i, line_idx in reversed(list(enumerate(candidate))):\n",
    "        if len(lines[line_idx].strip()) > TOO_LONG_SIGNATURE_LINE:\n",
    "            markers[i] = 'l'\n",
    "        else:\n",
    "            line = lines[line_idx].strip()\n",
    "            if line.startswith('-') and line.strip(\"-\"):\n",
    "                markers[i] = 'd'\n",
    "\n",
    "    return \"\".join(markers)\n",
    "\n",
    "\n",
    "def _process_marked_candidate_indexes(candidate, markers):\n",
    "    \"\"\"\n",
    "    Run regexes against candidate's marked indexes to strip\n",
    "    signature candidate.\n",
    "    >>> _process_marked_candidate_indexes([9, 12, 14, 15, 17], 'clddc')\n",
    "    [15, 17]\n",
    "    \"\"\"\n",
    "    match = RE_SIGNATURE_CANDIDATE.match(markers[::-1])\n",
    "    return candidate[-match.end('candidate'):] if match else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hi Jane,\n",
      "\n",
      "Thank you for keeping me updated on this issue. I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. \n",
      "Also many thanks for your suggestions. We hope to improve this feature in the future. \n",
      "\n",
      "In case you experience any further problems with the app, please don't hesitate to contact me again.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "John Doe\n",
      "Customer Support\n",
      "\n",
      "1600 Amphitheatre Parkway\n",
      "Mountain View, CA\n",
      "United States\n",
      "Best regards,\n",
      "\n",
      "John Doe\n",
      "Customer Support\n",
      "\n",
      "1600 Amphitheatre Parkway\n",
      "Mountain View, CA\n",
      "United States\n"
     ]
    }
   ],
   "source": [
    "Cleaned_Email=extract_signature(Email_EnglishText)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = Cleaned_Email.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Jane,',\n",
       " '',\n",
       " \"Thank you for keeping me updated on this issue. I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. \",\n",
       " 'Also many thanks for your suggestions. We hope to improve this feature in the future. ',\n",
       " '',\n",
       " \"In case you experience any further problems with the app, please don't hesitate to contact me again.\"]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line for line in lines if line != '']\n",
    "cleaned_email = ' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Jane, Thank you for keeping me updated on this issue. I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again.  Also many thanks for your suggestions. We hope to improve this feature in the future.  In case you experience any further problems with the app, please don't hesitate to contact me again.\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from talon.signature.bruteforce import extract_signature\n",
    "cleaned_email1, _ = extract_signature(Email_EnglishText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Jane,\\n\\nThank you for keeping me updated on this issue. I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. \\nAlso many thanks for your suggestions. We hope to improve this feature in the future. \\n\\nIn case you experience any further problems with the app, please don't hesitate to contact me again.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_email1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "### Language detection....\n",
    "from langdetect import detect\n",
    "lang = detect(cleaned_email) # lang = 'en' for an English email\n",
    "print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sbha69\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Jane,\\n\\nThank you for keeping me updated on this issue.',\n",
       " \"I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again.\",\n",
       " 'Also many thanks for your suggestions.',\n",
       " 'We hope to improve this feature in the future.',\n",
       " \"In case you experience any further problems with the app, please don't hesitate to contact me again.\",\n",
       " 'Best regards,\\n\\nJohn Doe\\nCustomer Support\\n\\n1600 Amphitheatre Parkway\\nMountain View, CA\\nUnited States']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sentence tokenization\n",
    "#Once the languages identification is performed for every email, we can use this information to split each email into its \n",
    "#constituent sentences using specific rules for sentence delimiters for each language.\n",
    "import nltk.data\n",
    "\n",
    "import sys\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "p= sys.stdin.read()\n",
    "\n",
    "sentences=tokenizer.tokenize(Email_EnglishText)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence tokenization\n",
    "#Once the languages identification is performed for every email, we can use this information to split each email into its constituent sentences using specific rules for sentence delimiters for each language.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(Email_EnglishText, language = lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Jane,\\n\\nThank you for keeping me updated on this issue. I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. \\nAlso many thanks for your suggestions. We hope to improve this feature in the future. \\n\\nIn case you experience any further problems with the app, please don't hesitate to contact me again.\\n\\nBest regards,\\n\\nJohn Doe\\nCustomer Support\\n\\n1600 Amphitheatre Parkway\\nMountain View, CA\\nUnited States\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_EnglishText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: skipthoughts in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from skipthoughts) (0.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sbha69\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from skipthoughts) (1.14.2)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'skipthoughts' has no attribute 'load_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-f98b7a3581a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# You would need to download pre-trained models first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskipthoughts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'skipthoughts' has no attribute 'load_model'"
     ]
    }
   ],
   "source": [
    "## Skip-thought Encoder...\n",
    "# The 'skipthoughts' module can be found at the root of the GitHub  repository linked above\n",
    "!pip3 install skipthoughts\n",
    "import skipthoughts\n",
    "\n",
    "# You would need to download pre-trained models first\n",
    "model = skipthoughts.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
